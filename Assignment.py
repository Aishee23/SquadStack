# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qYZizWQZnRc81qLHDqupFwARUJzMG59s
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# For model building
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, roc_curve, accuracy_score

# Loading the dataset
df = pd.read_csv('Training Data Dump - Dump.csv')  # Replace with your file path


print(df.info())
print(df.describe())
print(df.head())

print(df.isnull().sum())

missing_data_summary = df.isnull().sum().sort_values(ascending=False)
print("Missing Data Summary:\n", missing_data_summary)

# Dropping columns with >95% missing values
threshold = 0.95
columns_to_drop = missing_data_summary[missing_data_summary > len(df) * threshold].index
data_cleaned = df.drop(columns=columns_to_drop)

# Median imputation for numerical features
numerical_columns = data_cleaned.select_dtypes(include=['float64', 'int64']).columns
for col in numerical_columns:
    data_cleaned[col].fillna(data_cleaned[col].median(), inplace=True)

# Imputation for categorical columns
categorical_columns = data_cleaned.select_dtypes(include=['object']).columns
for col in categorical_columns:
    data_cleaned[col].fillna('Unknown', inplace=True)

# Verifying no missing values remain
print("Remaining missing values:", data_cleaned.isnull().sum().sum())

# Visualizing target variable distribution
plt.figure(figsize=(6, 4))
sns.countplot(x='Label', data=data_cleaned, palette='viridis')
plt.title('Distribution of Target Variable (Label)')
plt.xlabel('Label (0 = Not Converted, 1 = Converted)')
plt.ylabel('Count')
plt.show()

# Correlation heatmap for numerical features
plt.figure(figsize=(10, 8))
correlation_matrix = data_cleaned[numerical_columns].corr()
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')
plt.title('Correlation Heatmap of Numerical Features')
plt.show()

# Separate Lead_ID, features, and target
lead_ids = data_cleaned['Lead_ID']  # Retain Lead_ID for later use
X = data_cleaned.drop(columns=['Lead_ID', 'Label'])  # Features
y = data_cleaned['Label']  # Target variable

# Performing stratified train-test split
X_train, X_test, y_train, y_test, lead_ids_train, lead_ids_test = train_test_split(
    X, y, lead_ids, test_size=0.2, stratify=y, random_state=42
)

# Shapes of datasets
print("Training set shape:", X_train.shape)
print("Test set shape:", X_test.shape)
print("Lead_IDs test shape:", lead_ids_test.shape)

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Separating numerical and categorical columns
numerical_columns = X.select_dtypes(include=['float64', 'int64']).columns
categorical_columns = X.select_dtypes(include=['object']).columns

# Preprocessing for numerical data: fill missing values
numerical_transformer = SimpleImputer(strategy='median')

# Preprocessing for categorical data: fill missing values and encode
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combining preprocessors
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_columns),
        ('cat', categorical_transformer, categorical_columns)
    ]
)

# Integrating preprocessing into a pipeline with Logistic Regression
log_reg_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=1000, random_state=42))
])

# Fitting the pipeline on the training data
log_reg_pipeline.fit(X_train, y_train)

# Predictions
y_pred_lr = log_reg_pipeline.predict(X_test)
y_pred_proba_lr = log_reg_pipeline.predict_proba(X_test)[:, 1]

# Evaluating performance
print("Logistic Regression Performance:")
print(classification_report(y_test, y_pred_lr))
print("ROC-AUC:", roc_auc_score(y_test, y_pred_proba_lr))

# Decision Tree Pipeline
dt_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', DecisionTreeClassifier(max_depth=5, random_state=42))
])

# Fitting the pipeline on the training data
dt_pipeline.fit(X_train, y_train)

# Predictions
y_pred_dt = dt_pipeline.predict(X_test)
y_pred_proba_dt = dt_pipeline.predict_proba(X_test)[:, 1]

# Evaluating performance
print("\nDecision Tree Performance:")
print(classification_report(y_test, y_pred_dt))
print("ROC-AUC:", roc_auc_score(y_test, y_pred_proba_dt))

# One-hot encoded feature names
onehot_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_columns)
all_feature_names = np.append(numerical_columns, onehot_feature_names)

# Decision Tree Feature Importances
feature_importances = pd.Series(dt_pipeline.named_steps['classifier'].feature_importances_, index=all_feature_names)
feature_importances = feature_importances.sort_values(ascending=False).head(10)

# Plotting Feature Importance
feature_importances.plot(kind='bar', figsize=(10, 6), title='Top 10 Feature Importances (Decision Tree)')
plt.ylabel('Importance')
plt.show()

from sklearn.ensemble import RandomForestClassifier

# Random Forest Pipeline
rf_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
])

# Fitting and evaluating
rf_pipeline.fit(X_train, y_train)
y_pred_rf = rf_pipeline.predict(X_test)
y_pred_proba_rf = rf_pipeline.predict_proba(X_test)[:, 1]

print("\nRandom Forest Performance:")
print(classification_report(y_test, y_pred_rf))
print("ROC-AUC:", roc_auc_score(y_test, y_pred_proba_rf))

from sklearn.svm import SVC

# SVM Pipeline (with probability=True for ROC-AUC)
svm_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', SVC(probability=True, random_state=42))
])

# Fitting and evaluating
svm_pipeline.fit(X_train, y_train)
y_pred_svm = svm_pipeline.predict(X_test)
y_pred_proba_svm = svm_pipeline.predict_proba(X_test)[:, 1]

print("\nSVM Performance:")
print(classification_report(y_test, y_pred_svm))
print("ROC-AUC:", roc_auc_score(y_test, y_pred_proba_svm))

# Comparing ROC-AUC
models = ['Logistic Regression', 'Decision Tree', 'Random Forest',  'SVM']
roc_auc_scores = [
    roc_auc_score(y_test, y_pred_proba_lr),
    roc_auc_score(y_test, y_pred_proba_dt),
    roc_auc_score(y_test, y_pred_proba_rf),
    roc_auc_score(y_test, y_pred_proba_svm)
]

# Plot comparison
plt.figure(figsize=(10, 6))
sns.barplot(x=models, y=roc_auc_scores, palette='viridis')
plt.title('Model Comparison: ROC-AUC Scores')
plt.ylabel('ROC-AUC')
plt.show()

roc_auc_table = pd.DataFrame({
    'Model': models,
    'ROC-AUC Score': roc_auc_scores
})

# Displaying the table
print(roc_auc_table)

# Saving the table to a CSV file if needed
roc_auc_table.to_csv('Model_ROC_AUC_Scores.csv', index=False)

#Creating a DataFrame with predictions and lead IDs
results = pd.DataFrame({
    'Lead_ID': lead_ids_test,
    'Predicted_Conversion': y_pred_rf,
    'Prediction_Probability': y_pred_proba_rf
})

# Saving to CSV
results.to_csv('Lead_Predictions.csv', index=False)

print("Predictions merged with Lead_ID and saved to Lead_Predictions.csv.")

